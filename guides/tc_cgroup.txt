limit process bandwidth by tc and cgroup

/usr/share/doc/kernel-doc-xxxx/Documentation/cgroups/net_cls.txt 
https://www.kernel.org/doc/Documentation/cgroup-v1/net_cls.txt
Example:
mkdir /sys/fs/cgroup/net_cls
mount -t cgroup -onet_cls net_cls /sys/fs/cgroup/net_cls
mkdir /sys/fs/cgroup/net_cls/0

for centos7 system you can ignore previous three steps becase it already mounted:
echo 0x100001 >  /sys/fs/cgroup/net_cls/0/net_cls.classid
	- setting a 10:1 handle.

cat /sys/fs/cgroup/net_cls/0/net_cls.classid
1048577
pgrep mongod|xargs -I k echo k > /sys/fs/cgroup/net_cls,net_prio/foobar/tasks 

configuring tc:
tc qdisc add dev eth0 root handle 10: htb

tc class add dev eth0 parent 10: classid 10:1 htb rate 100mbit
 - creating traffic class 10:1

tc filter add dev eth0 parent 10: protocol ip prio 10 handle 1: cgroup

tc -s -d qdisc show dev eth0
tc -s -d filter show dev eth0
tc -s -d class show dev eth0

rate and backlog stats on htb are always 0. I think the rate estimators in these distros are not enabled by default because, if you have too much htb classes it could consume much CPU resources.
Try to do as root once sch_htb module has been loaded
echo 1 > /sys/module/sch_htb/parameters/htb_rate_est
After giving the above command, replace your htb tree.


configuring iptables, basic example if need:
iptables -A OUTPUT -m cgroup ! --cgroup 0x100001 -j DROP





The following tips may help in choosing which queue to use:

To purely slow down outgoing traffic, use the Token Bucket Filter. Works up to huge bandwidths, if you scale the bucket.

If your link is truly full and you want to make sure that no single session can dominate your outgoing bandwidth, use Stochastical Fairness Queueing.

If you have a big backbone and know what you are doing, consider Random Early Drop (see Advanced chapter).

To 'shape' incoming traffic which you are not forwarding, use the Ingress Policer. Incoming shaping is called 'policing', by the way, not 'shaping'.

If you *are* forwarding it, use a TBF on the interface you are forwarding the data to. Unless you want to shape traffic that may go out over several interfaces, in which case the only common factor is the incoming interface. In that case use the Ingress Policer.

If you don't want to shape, but only want to see if your interface is so loaded that it has to queue, use the pfifo queue (not pfifo_fast). It lacks internal bands but does account the size of its backlog.

Finally - you can also do "social shaping". You may not always be able to use technology to achieve what you want. Users experience technical constraints as hostile. A kind word may also help with getting your bandwidth to be divided right!


